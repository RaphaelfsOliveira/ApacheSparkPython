{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/raphaeloliveira/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# O NLTK é um kit de ferramentas para trabalharmos com linguagem natural no Python\n",
    "# Vamos utilizar apenas as definições de Stop Words do NLTK para removê-las da nossa word cloud\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .master('local[*]') \\\n",
    "    .config('spark.driver.host', '127.0.0.1')\\\n",
    "    .appName('WordCloud')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trata_tweets(df):\n",
    "#     words = df \\\n",
    "#         .select(f.explode(f.split(f.lower('_c0'), \" \")) \\                     # Adicionei a função lower para colocar todo o texto em minúsculo O \"_c0\" é o nome da coluna do DataFrame que contém os dados\n",
    "#         .alias(\"word\")) \\\n",
    "#         .withColumn('word', f.regexp_replace('word', r'http\\S+', '')) \\       # Retira os endereço web. O HTTP está em maiúsculo por conta da função upper utilizada acima\n",
    "#         .withColumn('word', f.regexp_replace('word', r'@\\w+', '')) \\          # Remove os nomes dos usuário do Twitter (@nome_usuário)\n",
    "#         .withColumn('word', f.regexp_replace('word', 'rt', '')) \\             # Remove a marcação RT dos retweets\n",
    "#         .na.replace('', None) \\                                               # Transforma vazios em None\n",
    "#         .na.drop()                                                            # Retira os valor nulos\n",
    "#     return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trata_tweets(df):\n",
    "    words = df \\\n",
    "        .select(f.explode(f.split(f.lower('value'), \" \")) \\\n",
    "        .alias(\"word\")) \\\n",
    "        .withColumn('word', f.regexp_replace('word', r'http\\S+', '')) \\\n",
    "        .withColumn('word', f.regexp_replace('word', r'@\\w+', '')) \\\n",
    "        .withColumn('word', f.regexp_replace('word', 'rt', '')) \\\n",
    "        .na.replace('', None) \\\n",
    "        .na.drop()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] O método collect do Spark é uma action e neste ponto é bom dar destaque ao conceito de Lazy Evaluation do Spark. Em resumo, no Spark temos funções de transformação e ação. Quando utilizamos as transformations o Spark cria um plano de ação e não executa a tarefa imediatamente. Este plano de ação é executado apenas quando utilizamos uma action. Isso permite que o Spark gere um plano de execução otimizado, mesclando algumas transformações e até mesmo pulando algumas que sejam desnecessárias.\n",
    "\n",
    "[2] Documentação do wordcloud: https://amueller.github.io/word_cloud/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Unable to infer schema for Parquet. It must be specified manually.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     \u001b[39mtry\u001b[39;00m:      \u001b[39m# Este try/except foi colocado para tratar os erros que aparecem quando interrompemos o processo\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m         words \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mparquet(path, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m)          \u001b[39m# Lendo o conjunto de arquivos CSV na pasta /csv\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         words \u001b[39m=\u001b[39m trata_tweets(words)                                 \u001b[39m# Aplicando nossa função de tratamento\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         rows \u001b[39m=\u001b[39m words\u001b[39m.\u001b[39mcollect()                                      \u001b[39m# Transformando o DataFrame em uma lista de linhas [1]\u001b[39;00m\n",
      "File \u001b[0;32m~/Galpao/ApacheSparkPython/.spark_venv/lib/python3.9/site-packages/pyspark/sql/readwriter.py:364\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    353\u001b[0m int96RebaseMode \u001b[39m=\u001b[39m options\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mint96RebaseMode\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    354\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(\n\u001b[1;32m    355\u001b[0m     mergeSchema\u001b[39m=\u001b[39mmergeSchema,\n\u001b[1;32m    356\u001b[0m     pathGlobFilter\u001b[39m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m     int96RebaseMode\u001b[39m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    362\u001b[0m )\n\u001b[0;32m--> 364\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mparquet(_to_seq(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_spark\u001b[39m.\u001b[39;49m_sc, paths)))\n",
      "File \u001b[0;32m~/Galpao/ApacheSparkPython/.spark_venv/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/Galpao/ApacheSparkPython/.spark_venv/lib/python3.9/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Unable to infer schema for Parquet. It must be specified manually."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "stops = stopwords.words('portuguese')                               # Criando uma variável com as Stop Word em português. Se for utilizar tweets em inglês basta modificar 'portuguese' para 'english'\n",
    "stops.append('economia')\n",
    "\n",
    "plt.figure(figsize=(20, 10))                                        # Cria a figura e defini o tamanho dela (largura, altura)\n",
    "\n",
    "path = '../datalake/twitter/parquet/'\n",
    "\n",
    "while True:\n",
    "    try:      # Este try/except foi colocado para tratar os erros que aparecem quando interrompemos o processo\n",
    "        words = spark.read.parquet(path, encoding='utf-8')          # Lendo o conjunto de arquivos CSV na pasta /csv\n",
    "        words = trata_tweets(words)                                 # Aplicando nossa função de tratamento\n",
    "        rows = words.collect()                                      # Transformando o DataFrame em uma lista de linhas [1]\n",
    "        all_words = ''\n",
    "        for row in rows:\n",
    "            all_words = all_words + ' ' + row['word']\n",
    "\n",
    "        wordcloud = WordCloud(stopwords=stops,\n",
    "                              background_color=\"black\",\n",
    "                              width=1920,\n",
    "                              height=1080,\n",
    "                              max_words=100\n",
    "                              ).generate(all_words)                 # Word cloud simples. Mais detalhes em [2]\n",
    "\n",
    "        plt.cla()                                                   # Limpa os eixos do gráfico\n",
    "        plt.axis('off')                                             # Oculta as marcações dos eixos\n",
    "        plt.imshow(wordcloud)                                       # Utilizado para exibir os dados como uma imagem\n",
    "        display.display(plt.gcf())                                  # Mostrando a nossa word cloud no output do notebook\n",
    "        display.clear_output(wait=True)                             # Limpa o output do notebook\n",
    "        time.sleep(5)\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".spark_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5b2ab901d88f4846543140d0410d9e785392947c0feb291dee01b3b6d5be827"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
