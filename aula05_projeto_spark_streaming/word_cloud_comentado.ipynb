{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/raphaeloliveira/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# O NLTK é um kit de ferramentas para trabalharmos com linguagem natural no Python\n",
    "# Vamos utilizar apenas as definições de Stop Words do NLTK para removê-las da nossa word cloud\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .master('local[*]') \\\n",
    "    .config('spark.driver.host', '127.0.0.1')\\\n",
    "    .appName('WordCloud')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trata_tweets(df):\n",
    "#     words = df \\\n",
    "#         .select(f.explode(f.split(f.lower('_c0'), \" \")) \\                     # Adicionei a função lower para colocar todo o texto em minúsculo O \"_c0\" é o nome da coluna do DataFrame que contém os dados\n",
    "#         .alias(\"word\")) \\\n",
    "#         .withColumn('word', f.regexp_replace('word', r'http\\S+', '')) \\       # Retira os endereço web. O HTTP está em maiúsculo por conta da função upper utilizada acima\n",
    "#         .withColumn('word', f.regexp_replace('word', r'@\\w+', '')) \\          # Remove os nomes dos usuário do Twitter (@nome_usuário)\n",
    "#         .withColumn('word', f.regexp_replace('word', 'rt', '')) \\             # Remove a marcação RT dos retweets\n",
    "#         .na.replace('', None) \\                                               # Transforma vazios em None\n",
    "#         .na.drop()                                                            # Retira os valor nulos\n",
    "#     return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trata_tweets(df):\n",
    "    words = df \\\n",
    "        .select(f.explode(f.split(f.lower('value'), \" \")) \\\n",
    "        .alias(\"word\")) \\\n",
    "        .withColumn('word', f.regexp_replace('word', r'http\\S+', '')) \\\n",
    "        .withColumn('word', f.regexp_replace('word', r'@\\w+', '')) \\\n",
    "        .withColumn('word', f.regexp_replace('word', 'rt', '')) \\\n",
    "        .na.replace('', None) \\\n",
    "        .na.drop()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] O método collect do Spark é uma action e neste ponto é bom dar destaque ao conceito de Lazy Evaluation do Spark. Em resumo, no Spark temos funções de transformação e ação. Quando utilizamos as transformations o Spark cria um plano de ação e não executa a tarefa imediatamente. Este plano de ação é executado apenas quando utilizamos uma action. Isso permite que o Spark gere um plano de execução otimizado, mesclando algumas transformações e até mesmo pulando algumas que sejam desnecessárias.\n",
    "\n",
    "[2] Documentação do wordcloud: https://amueller.github.io/word_cloud/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/Users/raphaeloliveira/Galpao/ApacheSparkPython/datalake/twitter/parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     \u001b[39mtry\u001b[39;00m:      \u001b[39m# Este try/except foi colocado para tratar os erros que aparecem quando interrompemos o processo\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m         words \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mparquet(path, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m)          \u001b[39m# Lendo o conjunto de arquivos CSV na pasta /csv\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         words \u001b[39m=\u001b[39m trata_tweets(words)                                 \u001b[39m# Aplicando nossa função de tratamento\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         rows \u001b[39m=\u001b[39m words\u001b[39m.\u001b[39mcollect()                                      \u001b[39m# Transformando o DataFrame em uma lista de linhas [1]\u001b[39;00m\n",
      "File \u001b[0;32m~/Galpao/ApacheSparkPython/.spark_venv/lib/python3.9/site-packages/pyspark/sql/readwriter.py:364\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    353\u001b[0m int96RebaseMode \u001b[39m=\u001b[39m options\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mint96RebaseMode\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    354\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(\n\u001b[1;32m    355\u001b[0m     mergeSchema\u001b[39m=\u001b[39mmergeSchema,\n\u001b[1;32m    356\u001b[0m     pathGlobFilter\u001b[39m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m     int96RebaseMode\u001b[39m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    362\u001b[0m )\n\u001b[0;32m--> 364\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mparquet(_to_seq(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_spark\u001b[39m.\u001b[39;49m_sc, paths)))\n",
      "File \u001b[0;32m~/Galpao/ApacheSparkPython/.spark_venv/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/Galpao/ApacheSparkPython/.spark_venv/lib/python3.9/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/Users/raphaeloliveira/Galpao/ApacheSparkPython/datalake/twitter/parquet"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function flush_figures at 0x11a05b280> (for post_execute):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Galpao/ApacheSparkPython/.spark_venv/lib/python3.9/site-packages/PIL/ImageFile.py:518\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 518\u001b[0m     fh \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mfileno()\n\u001b[1;32m    519\u001b[0m     fp\u001b[39m.\u001b[39mflush()\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Galpao/ApacheSparkPython/.spark_venv/lib/python3.9/site-packages/matplotlib_inline/backend_inline.py:126\u001b[0m, in \u001b[0;36mflush_figures\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mif\u001b[39;00m InlineBackend\u001b[39m.\u001b[39minstance()\u001b[39m.\u001b[39mclose_figures:\n\u001b[1;32m    124\u001b[0m     \u001b[39m# ignore the tracking, just draw and close all figures\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m         \u001b[39mreturn\u001b[39;00m show(\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    127\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    128\u001b[0m         \u001b[39m# safely show traceback if in IPython, else raise\u001b[39;00m\n\u001b[1;32m    129\u001b[0m         ip \u001b[39m=\u001b[39m get_ipython()\n",
      "File \u001b[0;32m~/Galpao/ApacheSparkPython/.spark_venv/lib/python3.9/site-packages/matplotlib_inline/backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[39mfor\u001b[39;00m figure_manager \u001b[39min\u001b[39;00m Gcf\u001b[39m.\u001b[39mget_all_fig_managers():\n\u001b[0;32m---> 90\u001b[0m         display(\n\u001b[1;32m     91\u001b[0m             figure_manager\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mfigure,\n\u001b[1;32m     92\u001b[0m             metadata\u001b[39m=\u001b[39;49m_fetch_figure_metadata(figure_manager\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mfigure)\n\u001b[1;32m     93\u001b[0m         )\n\u001b[1;32m     94\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     show\u001b[39m.\u001b[39m_to_draw \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/Galpao/ApacheSparkPython/.spark_venv/lib/python3.9/site-packages/IPython/core/display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     publish_display_data(data\u001b[39m=\u001b[39mobj, metadata\u001b[39m=\u001b[39mmetadata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     format_dict, md_dict \u001b[39m=\u001b[39m \u001b[39mformat\u001b[39;49m(obj, include\u001b[39m=\u001b[39;49minclude, exclude\u001b[39m=\u001b[39;49mexclude)\n\u001b[1;32m    299\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m format_dict:\n\u001b[1;32m    300\u001b[0m         \u001b[39m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/Galpao/ApacheSparkPython/.spark_venv/lib/python3.9/site-packages/IPython/core/formatters.py:177\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    175\u001b[0m md \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     data \u001b[39m=\u001b[39m formatter(obj)\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[39m# FIXME: log the exception\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/Galpao/ApacheSparkPython/.spark_venv/lib/python3.9/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[39m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[39mreturn\u001b[39;00m caller(func, \u001b[39m*\u001b[39;49m(extras \u001b[39m+\u001b[39;49m args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/Galpao/ApacheSparkPython/.spark_venv/lib/python3.9/site-packages/IPython/core/formatters.py:221\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     r \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    222\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     \u001b[39m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_return(\u001b[39mNone\u001b[39;00m, args[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/Galpao/ApacheSparkPython/.spark_venv/lib/python3.9/site-packages/IPython/core/formatters.py:338\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 338\u001b[0m     \u001b[39mreturn\u001b[39;00m printer(obj)\n\u001b[1;32m    339\u001b[0m \u001b[39m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    340\u001b[0m method \u001b[39m=\u001b[39m get_real_method(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_method)\n",
      "File \u001b[0;32m~/Galpao/ApacheSparkPython/.spark_venv/lib/python3.9/site-packages/IPython/core/pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend_bases\u001b[39;00m \u001b[39mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 152\u001b[0m fig\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mprint_figure(bytes_io, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    153\u001b[0m data \u001b[39m=\u001b[39m bytes_io\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    154\u001b[0m \u001b[39mif\u001b[39;00m fmt \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msvg\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m~/Galpao/ApacheSparkPython/.spark_venv/lib/python3.9/site-packages/matplotlib/backend_bases.py:2362\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2358\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2359\u001b[0m     \u001b[39m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[1;32m   2360\u001b[0m     \u001b[39m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[1;32m   2361\u001b[0m     \u001b[39mwith\u001b[39;00m cbook\u001b[39m.\u001b[39m_setattr_cm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure, dpi\u001b[39m=\u001b[39mdpi):\n\u001b[0;32m-> 2362\u001b[0m         result \u001b[39m=\u001b[39m print_method(\n\u001b[1;32m   2363\u001b[0m             filename,\n\u001b[1;32m   2364\u001b[0m             facecolor\u001b[39m=\u001b[39;49mfacecolor,\n\u001b[1;32m   2365\u001b[0m             edgecolor\u001b[39m=\u001b[39;49medgecolor,\n\u001b[1;32m   2366\u001b[0m             orientation\u001b[39m=\u001b[39;49morientation,\n\u001b[1;32m   2367\u001b[0m             bbox_inches_restore\u001b[39m=\u001b[39;49m_bbox_inches_restore,\n\u001b[1;32m   2368\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2369\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   2370\u001b[0m     \u001b[39mif\u001b[39;00m bbox_inches \u001b[39mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[0;32m~/Galpao/ApacheSparkPython/.spark_venv/lib/python3.9/site-packages/matplotlib/backend_bases.py:2228\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2224\u001b[0m     optional_kws \u001b[39m=\u001b[39m {  \u001b[39m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[1;32m   2225\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdpi\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfacecolor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39medgecolor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39morientation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2226\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbbox_inches_restore\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m   2227\u001b[0m     skip \u001b[39m=\u001b[39m optional_kws \u001b[39m-\u001b[39m {\u001b[39m*\u001b[39minspect\u001b[39m.\u001b[39msignature(meth)\u001b[39m.\u001b[39mparameters}\n\u001b[0;32m-> 2228\u001b[0m     print_method \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mwraps(meth)(\u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: meth(\n\u001b[1;32m   2229\u001b[0m         \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{k: v \u001b[39mfor\u001b[39;49;00m k, v \u001b[39min\u001b[39;49;00m kwargs\u001b[39m.\u001b[39;49mitems() \u001b[39mif\u001b[39;49;00m k \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m skip}))\n\u001b[1;32m   2230\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     print_method \u001b[39m=\u001b[39m meth\n",
      "File \u001b[0;32m~/Galpao/ApacheSparkPython/.spark_venv/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:509\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprint_png\u001b[39m(\u001b[39mself\u001b[39m, filename_or_obj, \u001b[39m*\u001b[39m, metadata\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, pil_kwargs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    463\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[39m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[39m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_print_pil(filename_or_obj, \u001b[39m\"\u001b[39;49m\u001b[39mpng\u001b[39;49m\u001b[39m\"\u001b[39;49m, pil_kwargs, metadata)\n",
      "File \u001b[0;32m~/Galpao/ApacheSparkPython/.spark_venv/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:458\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[39mDraw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[39m*pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    457\u001b[0m FigureCanvasAgg\u001b[39m.\u001b[39mdraw(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 458\u001b[0m mpl\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mimsave(\n\u001b[1;32m    459\u001b[0m     filename_or_obj, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuffer_rgba(), \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49mfmt, origin\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mupper\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    460\u001b[0m     dpi\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49mdpi, metadata\u001b[39m=\u001b[39;49mmetadata, pil_kwargs\u001b[39m=\u001b[39;49mpil_kwargs)\n",
      "File \u001b[0;32m~/Galpao/ApacheSparkPython/.spark_venv/lib/python3.9/site-packages/matplotlib/image.py:1687\u001b[0m, in \u001b[0;36mimsave\u001b[0;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m   1685\u001b[0m pil_kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mformat\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mformat\u001b[39m)\n\u001b[1;32m   1686\u001b[0m pil_kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mdpi\u001b[39m\u001b[39m\"\u001b[39m, (dpi, dpi))\n\u001b[0;32m-> 1687\u001b[0m image\u001b[39m.\u001b[39;49msave(fname, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpil_kwargs)\n",
      "File \u001b[0;32m~/Galpao/ApacheSparkPython/.spark_venv/lib/python3.9/site-packages/PIL/Image.py:2431\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2428\u001b[0m         fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39mopen(filename, \u001b[39m\"\u001b[39m\u001b[39mw+b\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2430\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2431\u001b[0m     save_handler(\u001b[39mself\u001b[39;49m, fp, filename)\n\u001b[1;32m   2432\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   2433\u001b[0m     \u001b[39mif\u001b[39;00m open_fp:\n",
      "File \u001b[0;32m~/Galpao/ApacheSparkPython/.spark_venv/lib/python3.9/site-packages/PIL/PngImagePlugin.py:1420\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1418\u001b[0m     _write_multiple_frames(im, fp, chunk, rawmode, default_image, append_images)\n\u001b[1;32m   1419\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1420\u001b[0m     ImageFile\u001b[39m.\u001b[39;49m_save(im, _idat(fp, chunk), [(\u001b[39m\"\u001b[39;49m\u001b[39mzip\u001b[39;49m\u001b[39m\"\u001b[39;49m, (\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m) \u001b[39m+\u001b[39;49m im\u001b[39m.\u001b[39;49msize, \u001b[39m0\u001b[39;49m, rawmode)])\n\u001b[1;32m   1422\u001b[0m \u001b[39mif\u001b[39;00m info:\n\u001b[1;32m   1423\u001b[0m     \u001b[39mfor\u001b[39;00m info_chunk \u001b[39min\u001b[39;00m info\u001b[39m.\u001b[39mchunks:\n",
      "File \u001b[0;32m~/Galpao/ApacheSparkPython/.spark_venv/lib/python3.9/site-packages/PIL/ImageFile.py:522\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    520\u001b[0m     _encode_tile(im, fp, tile, bufsize, fh)\n\u001b[1;32m    521\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mAttributeError\u001b[39;00m, io\u001b[39m.\u001b[39mUnsupportedOperation) \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m--> 522\u001b[0m     _encode_tile(im, fp, tile, bufsize, \u001b[39mNone\u001b[39;49;00m, exc)\n\u001b[1;32m    523\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(fp, \u001b[39m\"\u001b[39m\u001b[39mflush\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    524\u001b[0m     fp\u001b[39m.\u001b[39mflush()\n",
      "File \u001b[0;32m~/Galpao/ApacheSparkPython/.spark_venv/lib/python3.9/site-packages/PIL/ImageFile.py:541\u001b[0m, in \u001b[0;36m_encode_tile\u001b[0;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[39mif\u001b[39;00m exc:\n\u001b[1;32m    539\u001b[0m     \u001b[39m# compress to Python file-compatible object\u001b[39;00m\n\u001b[1;32m    540\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 541\u001b[0m         l, s, d \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39;49mencode(bufsize)\n\u001b[1;32m    542\u001b[0m         fp\u001b[39m.\u001b[39mwrite(d)\n\u001b[1;32m    543\u001b[0m         \u001b[39mif\u001b[39;00m s:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stops = stopwords.words('portuguese')                               # Criando uma variável com as Stop Word em português. Se for utilizar tweets em inglês basta modificar 'portuguese' para 'english'\n",
    "stops.append('carnaval')                                             # Pode ser interessante retirar a palavra utilizada na pesquisa\n",
    "plt.figure(figsize=(20, 10))                                        # Cria a figura e defini o tamanho dela (largura, altura)\n",
    "\n",
    "path = '../datalake/twitter/parquet/'\n",
    "\n",
    "while True:\n",
    "    try:      # Este try/except foi colocado para tratar os erros que aparecem quando interrompemos o processo\n",
    "        words = spark.read.parquet(path, encoding='utf-8')          # Lendo o conjunto de arquivos CSV na pasta /csv\n",
    "        words = trata_tweets(words)                                 # Aplicando nossa função de tratamento\n",
    "        rows = words.collect()                                      # Transformando o DataFrame em uma lista de linhas [1]\n",
    "        all_words = ''\n",
    "        for row in rows:\n",
    "            all_words = all_words + ' ' + row['word']\n",
    "\n",
    "        wordcloud = WordCloud(stopwords=stops,\n",
    "                              background_color=\"black\",\n",
    "                              width=1920,\n",
    "                              height=1080,\n",
    "                              max_words=100\n",
    "                              ).generate(all_words)                 # Word cloud simples. Mais detalhes em [2]\n",
    "\n",
    "        plt.cla()                                                   # Limpa os eixos do gráfico\n",
    "        plt.axis('off')                                             # Oculta as marcações dos eixos\n",
    "        plt.imshow(wordcloud)                                       # Utilizado para exibir os dados como uma imagem\n",
    "        display.display(plt.gcf())                                  # Mostrando a nossa word cloud no output do notebook\n",
    "        display.clear_output(wait=True)                             # Limpa o output do notebook\n",
    "        time.sleep(5)\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".spark_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5b2ab901d88f4846543140d0410d9e785392947c0feb291dee01b3b6d5be827"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
